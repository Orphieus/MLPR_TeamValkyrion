{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izaYzuVb8tNW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 150\n",
        "EMBEDDING_DIM = 300\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and preprocess dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['Utterance', 'Emotion'])\n",
        "    df['Utterance'] = df['Utterance'].str.lower()\n",
        "\n",
        "    # Emotion mapping\n",
        "    emotion_map = {'neutral':0, 'joy':1, 'sadness':2, 'anger':3,\n",
        "                   'surprise':4, 'disgust':5, 'fear':6}\n",
        "    df['label'] = df['Emotion'].map(emotion_map).dropna().astype(int)\n",
        "\n",
        "    return df['Utterance'], df['label']\n",
        "\n",
        "def create_embeddings(tokenizer):\n",
        "    \"\"\"Load pre-trained GloVe embeddings\"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open('glove.6B.300d.txt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= VOCAB_SIZE:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def build_cnn_bilstm(embedding_matrix):\n",
        "    \"\"\"Build CNN-BiLSTM model with regularization\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=MAX_LEN,\n",
        "                        trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_random_forest(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate Random Forest classifier\"\"\"\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=200,\n",
        "                                class_weight='balanced',\n",
        "                                n_jobs=-1)\n",
        "    rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    y_pred = rf.predict(X_test_tfidf)\n",
        "    print(\"Random Forest Performance:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Macro F1: {f1_score(y_test, y_pred, average='macro'):.4f}\\n\")\n",
        "    return tfidf, rf\n",
        "\n",
        "def test_new_dataset(model, tokenizer, tfidf_vectorizer, rf_model, test_file_path):\n",
        "    \"\"\"Evaluate models on new test dataset\"\"\"\n",
        "    # Load and preprocess test data\n",
        "    test_texts, test_labels = load_data(test_file_path)\n",
        "\n",
        "    # CNN-BiLSTM Evaluation\n",
        "    test_seq = tokenizer.texts_to_sequences(test_texts)\n",
        "    test_pad = pad_sequences(test_seq, maxlen=MAX_LEN)\n",
        "\n",
        "    print(\"\\nCNN-BiLSTM Performance on New Dataset:\")\n",
        "    y_probs = model.predict(test_pad)\n",
        "    y_pred = np.argmax(y_probs, axis=1)\n",
        "    print(classification_report(test_labels, y_pred))\n",
        "    print(f\"Macro F1: {f1_score(test_labels, y_pred, average='macro'):.4f}\")\n",
        "\n",
        "    # Random Forest Evaluation\n",
        "    print(\"\\nRandom Forest Performance on New Dataset:\")\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
        "    y_pred_rf = rf_model.predict(X_test_tfidf)\n",
        "    print(classification_report(test_labels, y_pred_rf))\n",
        "    print(f\"Macro F1: {f1_score(test_labels, y_pred_rf, average='macro'):.4f}\")\n",
        "\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    texts, labels = load_data('filtered_emotions_trimmed1.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Tokenization and sequencing\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    X_train_pad = pad_sequences(train_seq, maxlen=MAX_LEN)\n",
        "    X_test_pad = pad_sequences(test_seq, maxlen=MAX_LEN)\n",
        "\n",
        "    # Class weights for imbalance\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                        classes=np.unique(y_train),\n",
        "                                        y=y_train)\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    # Build and train CNN-BiLSTM\n",
        "    embedding_matrix = create_embeddings(tokenizer)\n",
        "    model = build_cnn_bilstm(embedding_matrix)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=2)\n",
        "    ]\n",
        "\n",
        "    print(\"Training CNN-BiLSTM Model:\")\n",
        "    history = model.fit(X_train_pad, y_train,\n",
        "                        epochs=15,\n",
        "                        batch_size=64,\n",
        "                        validation_split=0.1,\n",
        "                        class_weight=class_weights,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    # Evaluate deep learning model\n",
        "    print(\"\\nCNN-BiLSTM Performance:\")\n",
        "    y_probs = model.predict(X_test_pad)\n",
        "    y_pred = np.argmax(y_probs, axis=1)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Macro F1: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "\n",
        "    # Train and evaluate Random Forest\n",
        "    tfidf, rf = train_random_forest(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Test on new dataset\n",
        "    test_new_dataset(model, tokenizer, tfidf, rf, 'MELD Test Sent Emo.csv')  # Replace with your test file path\n",
        "\n",
        "    return model, tokenizer, tfidf, rf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cnn_model, tokenizer, tfidf_vectorizer, rf_model = main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBVDw5Ti8yQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 150\n",
        "EMBEDDING_DIM = 300\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and preprocess dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.dropna(subset=['Utterance', 'Emotion'])\n",
        "    df['Utterance'] = df['Utterance'].str.lower()\n",
        "    return df['Utterance'], df['Emotion']\n",
        "\n",
        "def create_embeddings(tokenizer):\n",
        "    \"\"\"Load pre-trained GloVe embeddings\"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open('glove.6B.300d.txt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= VOCAB_SIZE:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "def build_cnn_bilstm(embedding_matrix):\n",
        "    \"\"\"Build CNN-BiLSTM model with regularization\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=MAX_LEN,\n",
        "                        trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(7, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X, y, model_name=\"\"):\n",
        "    \"\"\"Generate detailed evaluation report\"\"\"\n",
        "    y_pred = np.argmax(model.predict(X), axis=1)\n",
        "    print(f\"\\n{model_name} Classification Report:\")\n",
        "    print(classification_report(y, y_pred))\n",
        "    print(f\"{model_name} Macro F1: {f1_score(y, y_pred, average='macro'):.4f}\")\n",
        "    print(f\"{model_name} Confusion Matrix:\")\n",
        "    print(confusion_matrix(y, y_pred))\n",
        "\n",
        "def test_new_dataset(model, tokenizer, label_encoder, tfidf_vectorizer=None, rf_model=None):\n",
        "    \"\"\"Evaluate models on new test dataset\"\"\"\n",
        "    # Load and preprocess new data\n",
        "    new_texts, new_emotions = load_data('MELD Test Sent Emo.csv')\n",
        "    new_labels = label_encoder.transform(new_emotions)\n",
        "\n",
        "    # CNN-BiLSTM evaluation\n",
        "    new_seq = tokenizer.texts_to_sequences(new_texts)\n",
        "    new_pad = pad_sequences(new_seq, maxlen=MAX_LEN)\n",
        "    evaluate_model(model, new_pad, new_labels, \"CNN-BiLSTM - New Dataset\")\n",
        "\n",
        "    # Random Forest evaluation\n",
        "    if rf_model and tfidf_vectorizer:\n",
        "        new_tfidf = tfidf_vectorizer.transform(new_texts)\n",
        "        y_pred_rf = rf_model.predict(new_tfidf)\n",
        "        print(\"\\nRandom Forest - New Dataset Classification Report:\")\n",
        "        print(classification_report(new_labels, y_pred_rf))\n",
        "        print(f\"Random Forest - New Dataset Macro F1: {f1_score(new_labels, y_pred_rf, average='macro'):.4f}\")\n",
        "\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    texts, emotions = load_data('filtered_emotions_trimmed1.csv')\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(emotions)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tokenization and sequencing\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    X_train_pad = pad_sequences(train_seq, maxlen=MAX_LEN)\n",
        "    X_test_pad = pad_sequences(test_seq, maxlen=MAX_LEN)\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    # Build and train CNN-BiLSTM\n",
        "    embedding_matrix = create_embeddings(tokenizer)\n",
        "    model = build_cnn_bilstm(embedding_matrix)\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=2)\n",
        "    ]\n",
        "    model.fit(X_train_pad, y_train,\n",
        "              epochs=15,\n",
        "              batch_size=64,\n",
        "              validation_split=0.1,\n",
        "              class_weight=class_weights,\n",
        "              callbacks=callbacks)\n",
        "\n",
        "    # Evaluate CNN-BiLSTM on test split\n",
        "    evaluate_model(model, X_test_pad, y_test, \"CNN-BiLSTM - Test Split\")\n",
        "\n",
        "    # Train and evaluate Random Forest\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "    rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', n_jobs=-1)\n",
        "    rf.fit(X_train_tfidf, y_train)\n",
        "    print(\"\\nRandom Forest - Test Split Classification Report:\")\n",
        "    print(classification_report(y_test, rf.predict(X_test_tfidf)))\n",
        "\n",
        "    # Evaluate on MELD test set\n",
        "    meld_texts, meld_emotions = load_data('MELD Test Sent Emo.csv')\n",
        "    meld_labels = label_encoder.transform(meld_emotions)\n",
        "    meld_seq = tokenizer.texts_to_sequences(meld_texts)\n",
        "    meld_pad = pad_sequences(meld_seq, maxlen=MAX_LEN)\n",
        "    evaluate_model(model, meld_pad, meld_labels, \"CNN-BiLSTM - MELD Test Set\")\n",
        "\n",
        "    # Test on new dataset\n",
        "    test_new_dataset(model, tokenizer, label_encoder, tfidf, rf)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "y94KmL8T8yOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpmsCE9c8yLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    return text.lower()\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space\n",
        "    params = {\n",
        "        'conv_filters': trial.suggest_categorical('conv_filters', [64, 128, 256]),\n",
        "        'kernel_size': trial.suggest_int('kernel_size', 3, 5),\n",
        "        'lstm_units': trial.suggest_categorical('lstm_units', [64, 128, 256]),\n",
        "        'dense_layers': trial.suggest_int('dense_layers', 1, 3),\n",
        "        'dropout_rate': trial.suggest_float('dropout_rate', 0.3, 0.6),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-3, log=True),\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
        "        'max_words': trial.suggest_int('max_words', 8000, 12000),\n",
        "        'max_len': trial.suggest_int('max_len', 80, 120)\n",
        "    }\n",
        "\n",
        "    # Sequence preprocessing\n",
        "    tokenizer = Tokenizer(num_words=params['max_words'], oov_token=\"<UNK>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=params['max_len'], padding='post', truncating='post')\n",
        "\n",
        "    # Train-val split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        padded_sequences,\n",
        "        categorical_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=categorical_labels\n",
        "    )\n",
        "\n",
        "    # CNN+BiLSTM architecture\n",
        "    input_layer = Input(shape=(params['max_len'],))\n",
        "    embedding = Embedding(input_dim=params['max_words'], output_dim=128)(input_layer)\n",
        "    conv = Conv1D(filters=params['conv_filters'], kernel_size=params['kernel_size'], activation='relu')(embedding)\n",
        "    bilstm = Bidirectional(LSTM(params['lstm_units']))(conv)\n",
        "\n",
        "    x = bilstm\n",
        "    for _ in range(params['dense_layers']):\n",
        "        x = Dense(trial.suggest_categorical(f'dense_units_{_}', [64, 128, 256]), activation='relu')(x)\n",
        "        x = Dropout(params['dropout_rate'])(x)\n",
        "\n",
        "    output_layer = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile and train\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=params['learning_rate']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=50,\n",
        "        batch_size=params['batch_size'],\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Validation metrics\n",
        "    val_preds = model.predict(X_val, verbose=0)\n",
        "    val_pred_classes = np.argmax(val_preds, axis=1)\n",
        "    val_true_classes = np.argmax(y_val, axis=1)\n",
        "    return f1_score(val_true_classes, val_pred_classes, average='weighted')\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    df = pd.read_csv('filtered_emotions_trimmed.csv', low_memory=False)\n",
        "    df['Utterance'] = df['Utterance'].apply(clean_text)\n",
        "    texts = df['Utterance'].values\n",
        "    labels = df['Emotion'].values\n",
        "\n",
        "    # Label encoding and class weights\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_labels = label_encoder.fit_transform(labels)\n",
        "    categorical_labels = to_categorical(integer_labels)\n",
        "\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                        classes=np.unique(integer_labels),\n",
        "                                        y=integer_labels)\n",
        "    class_weight_dict = dict(zip(range(len(class_weights)), class_weights))\n",
        "\n",
        "    # Hyperparameter optimization\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=20)\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # Final preprocessing with best parameters\n",
        "    tokenizer = Tokenizer(num_words=best_params['max_words'], oov_token=\"<UNK>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=best_params['max_len'], padding='post', truncating='post')\n",
        "\n",
        "    # Final model training\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        padded_sequences,\n",
        "        categorical_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=categorical_labels\n",
        "    )\n",
        "\n",
        "    # Build final model\n",
        "    input_layer = Input(shape=(best_params['max_len'],))\n",
        "    embedding = Embedding(input_dim=best_params['max_words'], output_dim=128)(input_layer)\n",
        "    conv = Conv1D(filters=best_params['conv_filters'], kernel_size=best_params['kernel_size'], activation='relu')(embedding)\n",
        "    bilstm = Bidirectional(LSTM(best_params['lstm_units']))(conv)\n",
        "\n",
        "    x = bilstm\n",
        "    for _ in range(best_params['dense_layers']):\n",
        "        x = Dense(best_params[f'dense_units_{_}'], activation='relu')(x)\n",
        "        x = Dropout(best_params['dropout_rate'])(x)\n",
        "\n",
        "    output_layer = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "    final_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    final_model.compile(\n",
        "        optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    final_model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=100,\n",
        "        batch_size=best_params['batch_size'],\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=15)],\n",
        "        class_weight=class_weight_dict\n",
        "    )\n",
        "\n",
        "    # Test evaluation\n",
        "    test_df = pd.read_csv('MELD Test Sent Emo.csv')\n",
        "    test_df['Utterance'] = test_df['Utterance'].apply(clean_text)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_df['Utterance'])\n",
        "    test_padded = pad_sequences(test_sequences, maxlen=best_params['max_len'], padding='post', truncating='post')\n",
        "    test_labels = label_encoder.transform(test_df['Emotion'])\n",
        "\n",
        "    test_preds = final_model.predict(test_padded)\n",
        "    test_pred_classes = np.argmax(test_preds, axis=1)\n",
        "\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    print(classification_report(test_labels, test_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "    print(\"\\nTest F1 Scores:\")\n",
        "    test_f1s = f1_score(test_labels, test_pred_classes, average=None)\n",
        "    for idx, label in enumerate(label_encoder.classes_):\n",
        "        print(f\"{label}: {test_f1s[idx]:.4f}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels, test_pred_classes))\n"
      ],
      "metadata": {
        "id": "EMbufs5Z8yJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f33930-f2bf-4bc8-9070-2c956436631b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 05:14:41,293] A new study created in memory with name: no-name-e7cca019-0250-4816-a43d-57edcaf7fbb0\n",
            "[I 2025-05-04 05:20:47,041] Trial 0 finished with value: 0.7765661739548247 and parameters: {'conv_filters': 64, 'kernel_size': 3, 'lstm_units': 128, 'dense_layers': 2, 'dropout_rate': 0.46566151330966354, 'learning_rate': 0.0001997430545776611, 'batch_size': 32, 'max_words': 9579, 'max_len': 95, 'dense_units_0': 256, 'dense_units_1': 128}. Best is trial 0 with value: 0.7765661739548247.\n",
            "[I 2025-05-04 05:26:12,208] Trial 1 finished with value: 0.7740190534850018 and parameters: {'conv_filters': 64, 'kernel_size': 5, 'lstm_units': 256, 'dense_layers': 3, 'dropout_rate': 0.3600361346065597, 'learning_rate': 0.0006532040049129005, 'batch_size': 64, 'max_words': 8445, 'max_len': 111, 'dense_units_0': 256, 'dense_units_1': 256, 'dense_units_2': 128}. Best is trial 0 with value: 0.7765661739548247.\n",
            "[I 2025-05-04 05:27:47,286] Trial 2 finished with value: 0.7838640024211949 and parameters: {'conv_filters': 128, 'kernel_size': 4, 'lstm_units': 64, 'dense_layers': 1, 'dropout_rate': 0.41047392147533, 'learning_rate': 0.0004312811176107819, 'batch_size': 128, 'max_words': 10725, 'max_len': 95, 'dense_units_0': 256}. Best is trial 2 with value: 0.7838640024211949.\n",
            "[I 2025-05-04 05:34:06,150] Trial 3 finished with value: 0.779819649707652 and parameters: {'conv_filters': 64, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.5475717043752178, 'learning_rate': 0.0002955820979803977, 'batch_size': 64, 'max_words': 11415, 'max_len': 107, 'dense_units_0': 64}. Best is trial 2 with value: 0.7838640024211949.\n",
            "[I 2025-05-04 05:37:14,381] Trial 4 finished with value: 0.7873728920991269 and parameters: {'conv_filters': 256, 'kernel_size': 3, 'lstm_units': 128, 'dense_layers': 3, 'dropout_rate': 0.47250862310776753, 'learning_rate': 0.000766257496156783, 'batch_size': 128, 'max_words': 10682, 'max_len': 89, 'dense_units_0': 128, 'dense_units_1': 128, 'dense_units_2': 256}. Best is trial 4 with value: 0.7873728920991269.\n",
            "[I 2025-05-04 05:42:17,527] Trial 5 finished with value: 0.7892996383486143 and parameters: {'conv_filters': 256, 'kernel_size': 3, 'lstm_units': 128, 'dense_layers': 1, 'dropout_rate': 0.3424134104310799, 'learning_rate': 0.00065543156189177, 'batch_size': 32, 'max_words': 9966, 'max_len': 89, 'dense_units_0': 128}. Best is trial 5 with value: 0.7892996383486143.\n",
            "[I 2025-05-04 05:48:21,538] Trial 6 finished with value: 0.7974211012534089 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.5147383876429694, 'learning_rate': 0.0005637248339858539, 'batch_size': 64, 'max_words': 9110, 'max_len': 113, 'dense_units_0': 64}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 05:51:25,144] Trial 7 finished with value: 0.7718936171469648 and parameters: {'conv_filters': 64, 'kernel_size': 3, 'lstm_units': 128, 'dense_layers': 3, 'dropout_rate': 0.38347357583163644, 'learning_rate': 0.00033567038771243186, 'batch_size': 128, 'max_words': 11942, 'max_len': 119, 'dense_units_0': 256, 'dense_units_1': 256, 'dense_units_2': 128}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 05:53:51,782] Trial 8 finished with value: 0.7536274336326656 and parameters: {'conv_filters': 64, 'kernel_size': 4, 'lstm_units': 128, 'dense_layers': 3, 'dropout_rate': 0.5578521552488356, 'learning_rate': 0.0001309647028855165, 'batch_size': 128, 'max_words': 11376, 'max_len': 91, 'dense_units_0': 256, 'dense_units_1': 256, 'dense_units_2': 256}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:01:04,078] Trial 9 finished with value: 0.7792962883946635 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 2, 'dropout_rate': 0.49244311012626896, 'learning_rate': 0.00016151708596016847, 'batch_size': 64, 'max_words': 9113, 'max_len': 88, 'dense_units_0': 128, 'dense_units_1': 256}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:04:07,789] Trial 10 finished with value: 0.7885605392117057 and parameters: {'conv_filters': 128, 'kernel_size': 5, 'lstm_units': 64, 'dense_layers': 2, 'dropout_rate': 0.5960051914502336, 'learning_rate': 0.0009119443482783826, 'batch_size': 64, 'max_words': 8570, 'max_len': 107, 'dense_units_0': 64, 'dense_units_1': 64}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:10:20,355] Trial 11 finished with value: 0.795511015176505 and parameters: {'conv_filters': 256, 'kernel_size': 3, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.311013507915761, 'learning_rate': 0.0005551130592529438, 'batch_size': 32, 'max_words': 9963, 'max_len': 83, 'dense_units_0': 128}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:16:18,180] Trial 12 finished with value: 0.7931771961382708 and parameters: {'conv_filters': 256, 'kernel_size': 5, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.3041097348472952, 'learning_rate': 0.0004590310656805487, 'batch_size': 32, 'max_words': 9251, 'max_len': 83, 'dense_units_0': 64}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:21:57,251] Trial 13 finished with value: 0.7852131429178953 and parameters: {'conv_filters': 256, 'kernel_size': 3, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.4254082268988735, 'learning_rate': 0.0004728070059675917, 'batch_size': 32, 'max_words': 8058, 'max_len': 80, 'dense_units_0': 64}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:28:11,540] Trial 14 finished with value: 0.7857714697771039 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 1, 'dropout_rate': 0.5299198160464671, 'learning_rate': 0.0002611448400174259, 'batch_size': 32, 'max_words': 10330, 'max_len': 119, 'dense_units_0': 128}. Best is trial 6 with value: 0.7974211012534089.\n",
            "[I 2025-05-04 06:34:40,595] Trial 15 finished with value: 0.7999613372607095 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 2, 'dropout_rate': 0.3019138425182825, 'learning_rate': 0.0009861013812737295, 'batch_size': 64, 'max_words': 9881, 'max_len': 102, 'dense_units_0': 64, 'dense_units_1': 64}. Best is trial 15 with value: 0.7999613372607095.\n",
            "[I 2025-05-04 06:37:48,454] Trial 16 finished with value: 0.7887259795400052 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 64, 'dense_layers': 2, 'dropout_rate': 0.5210441658816607, 'learning_rate': 0.0009116985399715906, 'batch_size': 64, 'max_words': 9118, 'max_len': 102, 'dense_units_0': 64, 'dense_units_1': 64}. Best is trial 15 with value: 0.7999613372607095.\n",
            "[I 2025-05-04 06:44:08,057] Trial 17 finished with value: 0.7956894641059294 and parameters: {'conv_filters': 128, 'kernel_size': 5, 'lstm_units': 256, 'dense_layers': 2, 'dropout_rate': 0.42606407986958306, 'learning_rate': 0.0009413210733891025, 'batch_size': 64, 'max_words': 9561, 'max_len': 114, 'dense_units_0': 64, 'dense_units_1': 64}. Best is trial 15 with value: 0.7999613372607095.\n",
            "[I 2025-05-04 06:49:23,899] Trial 18 finished with value: 0.793739182651739 and parameters: {'conv_filters': 256, 'kernel_size': 4, 'lstm_units': 256, 'dense_layers': 2, 'dropout_rate': 0.4982230673036852, 'learning_rate': 0.0007331538717644798, 'batch_size': 64, 'max_words': 8936, 'max_len': 101, 'dense_units_0': 64, 'dense_units_1': 64}. Best is trial 15 with value: 0.7999613372607095.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiuqLk00t3Nr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}